{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from xgboost) (2.2.4)\n",
      "Requirement already satisfied: scipy in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from xgboost) (1.15.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [11:59:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: Accuracy = 0.8670, F1 Score = 0.1827\n",
      "Best Params: {'n_estimators': 100, 'max_depth': None}\n",
      "------------------------------------------------------------\n",
      "XGBoost: Accuracy = 0.8774, F1 Score = 0.0089\n",
      "Best Params: {'n_estimators': 50, 'max_depth': 6, 'learning_rate': 0.2}\n",
      "------------------------------------------------------------\n",
      "Decision Tree: Accuracy = 0.8240, F1 Score = 0.2983\n",
      "Best Params: {'min_samples_split': 2, 'max_depth': None}\n",
      "------------------------------------------------------------\n",
      "Gradient Boosting: Accuracy = 0.8765, F1 Score = 0.0167\n",
      "Best Params: {'n_estimators': 50, 'max_depth': 6, 'learning_rate': 0.2}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"data.csv\")  # Ensure the correct path\n",
    "\n",
    "df.drop(columns=['id'], inplace=True)  # Drop ID column\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['Response'])\n",
    "y = df['Response']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Hyperparameter grids\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [10, 20, None]},\n",
    "    \"XGBoost\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [3, 6, 10], \"learning_rate\": [0.01, 0.1, 0.2]},\n",
    "    \"Decision Tree\": {\"max_depth\": [5, 10, 20, None], \"min_samples_split\": [2, 5, 10]},\n",
    "    \"Gradient Boosting\": {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 0.2], \"max_depth\": [3, 6, 10]}\n",
    "}\n",
    "\n",
    "# Train models with hyperparameter tuning\n",
    "best_models = {}\n",
    "model_performance = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    random_search = RandomizedSearchCV(model, param_grids[name], n_iter=5, scoring='f1', cv=3, random_state=42, n_jobs=-1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best model from tuning\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_models[name] = best_model\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    model_performance[name] = {\"Accuracy\": accuracy, \"F1 Score\": f1, \"Best Params\": random_search.best_params_}\n",
    "\n",
    "# Display best model performance\n",
    "for model, metrics in model_performance.items():\n",
    "    print(f\"{model}: Accuracy = {metrics['Accuracy']:.4f}, F1 Score = {metrics['F1 Score']:.4f}\")\n",
    "    print(f\"Best Params: {metrics['Best Params']}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from optuna) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 13:51:50,582] A new study created in memory with name: no-name-322eef11-5ff4-4f39-a08c-c230e7786eb4\n",
      "[I 2025-04-02 13:54:45,007] Trial 0 finished with value: 0.8757370950017597 and parameters: {'rf_n_estimators': 300, 'rf_max_depth': 45, 'rf_min_samples_split': 16, 'rf_min_samples_leaf': 7, 'rf_max_features': 'log2'}. Best is trial 0 with value: 0.8757370950017597.\n",
      "[I 2025-04-02 13:56:38,968] Trial 1 finished with value: 0.8726142475524776 and parameters: {'rf_n_estimators': 200, 'rf_max_depth': 45, 'rf_min_samples_split': 26, 'rf_min_samples_leaf': 8, 'rf_max_features': 'sqrt'}. Best is trial 0 with value: 0.8757370950017597.\n",
      "[I 2025-04-02 13:59:32,527] A new study created in memory with name: no-name-f11ac1ce-ef02-43db-82d9-0513cfe0734a\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/optuna/distributions.py:699: UserWarning: The distribution is specified by [3, 20] and step=2, but the range is not divisible by `step`. It will be replaced by [3, 19].\n",
      "  warnings.warn(\n",
      "[I 2025-04-02 13:59:34,183] Trial 0 finished with value: 0.8688490925442044 and parameters: {'xgb_n_estimators': 100, 'xgb_max_depth': 13, 'xgb_learning_rate': 0.028366715675972178, 'xgb_subsample': 0.5, 'xgb_colsample_bytree': 0.8}. Best is trial 0 with value: 0.8688490925442044.\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/optuna/distributions.py:699: UserWarning: The distribution is specified by [3, 20] and step=2, but the range is not divisible by `step`. It will be replaced by [3, 19].\n",
      "  warnings.warn(\n",
      "[I 2025-04-02 13:59:37,704] Trial 1 finished with value: 0.8790669212677956 and parameters: {'xgb_n_estimators': 400, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.15387091494428515, 'xgb_subsample': 0.7, 'xgb_colsample_bytree': 0.5}. Best is trial 1 with value: 0.8790669212677956.\n",
      "[I 2025-04-02 13:59:41,178] A new study created in memory with name: no-name-a3bbcceb-86fc-464f-87fe-c998e2340d84\n",
      "[I 2025-04-02 13:59:41,915] Trial 0 finished with value: 0.8339481925792239 and parameters: {'dt_max_depth': 5, 'dt_min_samples_split': 24, 'dt_min_samples_leaf': 14, 'dt_max_features': None}. Best is trial 0 with value: 0.8339481925792239.\n",
      "[I 2025-04-02 13:59:42,740] Trial 1 finished with value: 0.8460022542995309 and parameters: {'dt_max_depth': 35, 'dt_min_samples_split': 4, 'dt_min_samples_leaf': 9, 'dt_max_features': 'log2'}. Best is trial 1 with value: 0.8460022542995309.\n",
      "[I 2025-04-02 13:59:43,689] A new study created in memory with name: no-name-1901fa83-0424-4367-b013-3a04167206c9\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/optuna/distributions.py:699: UserWarning: The distribution is specified by [3, 20] and step=2, but the range is not divisible by `step`. It will be replaced by [3, 19].\n",
      "  warnings.warn(\n",
      "[I 2025-04-02 14:04:00,779] Trial 0 finished with value: 0.8856793818084561 and parameters: {'gb_n_estimators': 100, 'gb_learning_rate': 0.02416172643766952, 'gb_max_depth': 17, 'gb_subsample': 0.8, 'gb_min_samples_split': 22, 'gb_min_samples_leaf': 18}. Best is trial 0 with value: 0.8856793818084561.\n",
      "/Users/akashmukherjee/Programming/MLOPS/Vikas/Vechile-Insurance/my_env/lib/python3.11/site-packages/optuna/distributions.py:699: UserWarning: The distribution is specified by [3, 20] and step=2, but the range is not divisible by `step`. It will be replaced by [3, 19].\n",
      "  warnings.warn(\n",
      "[I 2025-04-02 14:06:45,408] Trial 1 finished with value: 0.8567236759631082 and parameters: {'gb_n_estimators': 150, 'gb_learning_rate': 0.014897904035005105, 'gb_max_depth': 7, 'gb_subsample': 1.0, 'gb_min_samples_split': 16, 'gb_min_samples_leaf': 8}. Best is trial 0 with value: 0.8856793818084561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: Accuracy = 0.8709, F1 Score = 0.8758\n",
      "Best Params: {'n_estimators': 300, 'max_depth': 45, 'min_samples_split': 16, 'min_samples_leaf': 7, 'max_features': 'log2'}\n",
      "------------------------------------------------------------\n",
      "XGBoost: Accuracy = 0.8774, F1 Score = 0.8791\n",
      "Best Params: {'n_estimators': 400, 'max_depth': 9, 'learning_rate': 0.15387091494428515, 'subsample': 0.7, 'colsample_bytree': 0.5}\n",
      "------------------------------------------------------------\n",
      "Decision Tree: Accuracy = 0.8481, F1 Score = 0.8515\n",
      "Best Params: {'max_depth': 35, 'min_samples_split': 4, 'min_samples_leaf': 9, 'max_features': 'log2'}\n",
      "------------------------------------------------------------\n",
      "Gradient Boosting: Accuracy = 0.8822, F1 Score = 0.8859\n",
      "Best Params: {'n_estimators': 100, 'learning_rate': 0.02416172643766952, 'max_depth': 17, 'subsample': 0.8, 'min_samples_split': 22, 'min_samples_leaf': 18}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"data.csv\")  # Ensure the correct path\n",
    "\n",
    "df.drop(columns=['id'], inplace=True)  # Drop ID column\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['Response'])\n",
    "y = df['Response']\n",
    "\n",
    "# Balance dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier,\n",
    "    \"XGBoost\": XGBClassifier,\n",
    "    \"Decision Tree\": DecisionTreeClassifier,\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier\n",
    "}\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial, model_name):\n",
    "    params = {}\n",
    "    if model_name == \"Random Forest\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"rf_n_estimators\", 50, 500, step=50),\n",
    "            \"max_depth\": trial.suggest_int(\"rf_max_depth\", 5, 50, step=5),\n",
    "            \"min_samples_split\": trial.suggest_int(\"rf_min_samples_split\", 2, 50, step=2),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"rf_min_samples_leaf\", 1, 20, step=1),\n",
    "            \"max_features\": trial.suggest_categorical(\"rf_max_features\", [\"sqrt\", \"log2\", None])\n",
    "        }\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 50, 500, step=50),\n",
    "            \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 20, step=2),\n",
    "            \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.5, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0, step=0.1),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0, step=0.1)\n",
    "        }\n",
    "    elif model_name == \"Decision Tree\":\n",
    "        params = {\n",
    "            \"max_depth\": trial.suggest_int(\"dt_max_depth\", 5, 50, step=5),\n",
    "            \"min_samples_split\": trial.suggest_int(\"dt_min_samples_split\", 2, 50, step=2),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"dt_min_samples_leaf\", 1, 20, step=1),\n",
    "            \"max_features\": trial.suggest_categorical(\"dt_max_features\", [\"sqrt\", \"log2\", None])\n",
    "        }\n",
    "    elif model_name == \"Gradient Boosting\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"gb_n_estimators\", 50, 500, step=50),\n",
    "            \"learning_rate\": trial.suggest_float(\"gb_learning_rate\", 0.01, 0.5, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"gb_max_depth\", 3, 20, step=2),\n",
    "            \"subsample\": trial.suggest_float(\"gb_subsample\", 0.5, 1.0, step=0.1),\n",
    "            \"min_samples_split\": trial.suggest_int(\"gb_min_samples_split\", 2, 50, step=2),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"gb_min_samples_leaf\", 1, 20, step=1)\n",
    "        }\n",
    "    \n",
    "    model = models[model_name](**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return f1_score(y_test, y_pred)\n",
    "\n",
    "# Run Optuna optimization\n",
    "best_models = {}\n",
    "model_performance = {}\n",
    "\n",
    "for model_name in models.keys():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, model_name), n_trials=2)\n",
    "    \n",
    "    best_params = {key.split('_', 1)[-1]: value for key, value in study.best_params.items()}\n",
    "    best_model = models[model_name](**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    best_models[model_name] = best_model\n",
    "    model_performance[model_name] = {\"Accuracy\": accuracy, \"F1 Score\": f1, \"Best Params\": best_params}\n",
    "\n",
    "# Display best model performance\n",
    "for model, metrics in model_performance.items():\n",
    "    print(f\"{model}: Accuracy = {metrics['Accuracy']:.4f}, F1 Score = {metrics['F1 Score']:.4f}\")\n",
    "    print(f\"Best Params: {metrics['Best Params']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
